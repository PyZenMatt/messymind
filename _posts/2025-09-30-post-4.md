---
title: post-4
---

---
title: "Mini-RAG su testi di mindfulness: dal pilot alla Policy GOLD v1.1 (case study operativo)"
description: "Due mini-RAG, una policy di valutazione promossa all’audit e una pipeline replicabile: chunking → BM25 → MiniLM → FAISS → RRF → GOLD v1.1 → metriche. Cosa ha funzionato, cosa no e come rifarlo per tutti i miei ebooks."
permalink: "mini-rag-case-study/"
slug: "mini-rag-mindfulness-policy-gold-v1-1"
date: 2025-09-22
last_modified_at: 2025-09-22
lang: it
categories: ["Progetti","ai","rag","case-study"]
tags: ["blog","seo","RAG","BM25","FAISS","MiniLM","Evaluation","Audit","Policy"]
canonical_url: "https://matteoricci.net/mini-rag-case-study/"
robots: "index, follow"
sitemap: true
published: false
---

# Mini-RAG su testi di mindfulness: dal pilot alla Policy GOLD v1.1

Costruire un RAG non è solo una questione di “far rispondere un modello”. È progettare una catena di passaggi che parte dai dati e arriva a risposte valutabili, con criteri espliciti e controllabili. In questo case study racconto come ho messo in piedi **due mini-RAG** su libri di mindfulness, come ho standardizzato la **Policy GOLD v1.1** e come ho verificato tutto con un audit umano leggero ma efficace.

Il risultato è una pipeline che si può **replicare** per tutti i libri senza impazzire.

### Table of Contents
- [Mini-RAG su testi di mindfulness: dal pilot alla Policy GOLD v1.1](#mini-rag-su-testi-di-mindfulness-dal-pilot-alla-policy-gold-v11)
    - [Table of Contents](#table-of-contents)
  - [Contesto: perché due indici separati](#contesto-perché-due-indici-separati)
  - [Il percorso, senza salti logici](#il-percorso-senza-salti-logici)
  - [Cosa dicono i numeri (senza fumo)](#cosa-dicono-i-numeri-senza-fumo)
  - [Dove il retrieval inciampa (e come l’ho visto)](#dove-il-retrieval-inciampa-e-come-lho-visto)
  - [Perché la policy batte le patch](#perché-la-policy-batte-le-patch)
  - [Artefatti che aiutano gli umani (non solo i modelli)](#artefatti-che-aiutano-gli-umani-non-solo-i-modelli)
  - [Limiti dichiarati (per scelta)](#limiti-dichiarati-per-scelta)
  - [Come replicare questo lavoro in 6 mosse](#come-replicare-questo-lavoro-in-6-mosse)
  - [Link utili e artefatti](#link-utili-e-artefatti)
  - [Cosa mi porto a casa](#cosa-mi-porto-a-casa)
    - [FAQ](#faq)
      - [Cos’è la Policy GOLD v1.1?](#cosè-la-policy-gold-v11)
      - [Perché usare due indici (BM25 e FAISS)?](#perché-usare-due-indici-bm25-e-faiss)
      - [Quali metriche ho usato?](#quali-metriche-ho-usato)
      - [Quando conviene un indice unificato?](#quando-conviene-un-indice-unificato)
    - [Apply This in Practice](#apply-this-in-practice)

## Contesto: perché due indici separati

Ho scelto di mantenere **indici separati per libro**. Non è una scelta “ideologica”: è controllo. Indici distinti significano analisi più pulita, regressioni più leggibili e meno rumore quando si correggono i borderline. L’unificazione arriverà, ma quando i singoli mattoni saranno solidi.

## Il percorso, senza salti logici

Sono partito dal testo e l’ho **normalizzato**, rispettando heading e coerenza dei passaggi. Da lì ho generato i `*.clean.chunks.jsonl` con manifest (conteggio, checksum, versioni). Ho poi costruito due indici: **BM25** per densità lessicale e **FAISS IndexFlatIP** con **embeddings MiniLM** (vettori normalizzati). In fase di retrieval uso **RRF (k=50)** per fondere i ranking. Per ogni query genero artefatti **human-readable** (Markdown e JSONL) che velocizzano l’audit.

Il cuore del controllo qualità è la **Policy GOLD**. Dalla v1.0 sono passato alla **v1.1**, che restringe i “positivi” borderline: niente meta/indice/bibliografia, niente semplici menzioni; conta il **testo che risponde davvero** all’intento della query. Poi ho fatto un **Audit-02**: un campione del 10% stratificato; correzioni ≤ 10% → promozione confermata.

## Cosa dicono i numeri (senza fumo)

- **Full Catastrophe Living (FCL)**: P@5 = **0,92**; Hit@1 = **1**
- **Satipaṭṭhāna (pilot-v1)**: P@5 **1,00 → 0,96** con v1.1, Hit@1 = **1**

In breve: v1.1 ripulisce i “quasi rilevanti” su Satipaṭṭhāna con un impatto minimo sul P@5; su FCL le metriche restano stabili.

## Dove il retrieval inciampa (e come l’ho visto)

Quando compare un **heading molto simile alla query** ma il corpo non aggiunge niente di utile, il ranking rischia di premiarlo. Idem per **meta/indice**: portano parole chiave, ma zero risposta pratica. Qui la v1.1 fa il suo dovere.

## Perché la policy batte le patch

Le patch (es. un post-filter sugli heading) possono aiutare, ma **prima** serve una **policy chiara**. La v1.1 ha risolto la maggior parte del rumore senza introdurre logiche opache.

## Artefatti che aiutano gli umani (non solo i modelli)

Per ogni query salvo un **Markdown leggibile** con il top-N, gli snippet completi e i riferimenti ai chunk. Governance: `INDEX.md`, `run_manifest.json`, `SUMMARY.md`, `GOLD_APPROVAL_v1.1.md`. Retention: **solo latest + best**.

## Limiti dichiarati (per scelta)

Niente indice unificato – per ora. Dopo Book-3 testerò cross-corpus o fusion multi-indice. Se emergono drift semantici, proverò **MPNet** su subset.

## Come replicare questo lavoro in 6 mosse

1. **Ingest & chunking**: normalizza testo, genera `*.clean.chunks.jsonl` + manifest.  
2. **Indici**: costruisci BM25 e embeddings MiniLM con FAISS IndexFlatIP.  
3. **Pilot**: lancia 5 query con RRF (k=50); esporta artefatti.  
4. **Valutazione**: auto-label con GOLD v1.1; Audit-02; calcolo P@5/Hit@1.  
5. **Governance**: aggiorna file chiave; mantieni latest + best.  
6. **Comparazione**: confronta Book-3 con Book-1/2 (failure modes).  

## Link utili e artefatti

- Satipaṭṭhāna — `reports/satipatthana/query_runs/pilot-v1/20250922_093323/`  
- FCL — `reports/full-catastrophe-living/query_runs/20250922_114412/`  
- File: `SUMMARY.md`, `q01..q05.md`, `queries.master.jsonl`, `GOLD_APPROVAL_v1.1.md`, `COMPARE_v1.0_vs_v1.1.md`

## Cosa mi porto a casa

Un RAG editoriale funziona quando è **misurabile**. La parte “intelligente” non è l’LLM: è il **processo** che lo circonda.

---

### FAQ

#### Cos’è la Policy GOLD v1.1?
È una policy di labeling che definisce cosa conta come “risposta valida”, riducendo i falsi positivi borderline.

#### Perché usare due indici (BM25 e FAISS)?
BM25 cattura densità lessicale, FAISS con embeddings MiniLM cattura semantica. La fusione RRF combina i due punti di forza.

#### Quali metriche ho usato?
P@5 e Hit@1, semplici ma efficaci per misurare precisione e copertura nei top risultati.

#### Quando conviene un indice unificato?
Solo dopo aver consolidato gli indici singoli, per evitare drift semantici e rumore non controllato.

---

### Apply This in Practice

- Segui la checklist in 6 mosse e replicala su un nuovo testo.  
- Documenta manifest, metriche e audit in file leggibili.  
- Confronta i risultati con casi precedenti per capire failure modes.  

Vuoi vedere un esempio completo? Dai un’occhiata a [Architettura minima per un mini-RAG](https://matteoricci.net/blog/architettura-mini-rag/).

Inoltre ti lascio qualche articolo da leggere dal mio blog di mindfulness!

* Verso il cluster *filosofia-pratica*: [Altri articoli su filosofia pratica](/categorie/filosofia-pratica/)
* Verso il subcluster *ai-e-coscienza*: [AI e coscienza: hub del subcluster](/categorie/filosofia-pratica/ai-e-coscienza/)
* Verso un fratello: [Costellazioni familiari: opinioni non mistiche](/filosofia-pratica/costellazioni-familiari-opinioni/)
* Verso un fratello 2: [Faggin e l’AI non cosciente](/filosofia-pratica/faggin-ia-non-cosciente/)
